{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "bc9589b8-b5fe-402a-8d55-846b9107942b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import gym\n",
    "from gym import spaces\n",
    "\n",
    "class FinancialPlanningEnv(gym.Env):\n",
    "    def __init__(self, goal=10000, bag_price=2000):\n",
    "        super(FinancialPlanningEnv, self).__init__()\n",
    "        self.state = None\n",
    "        self.savings = 0\n",
    "        self.total_income = 0\n",
    "        self.total_expenses = 0\n",
    "        self.goal = goal\n",
    "        self.bag_price = bag_price\n",
    "        self.max_daily_income = 159\n",
    "        self.min_daily_income = 136\n",
    "        self.max_daily_expenses = 35\n",
    "        self.min_daily_expenses = 12\n",
    "        self.action_space = spaces.Discrete(3)  # 0: save, 1: spend, 2: buy bag\n",
    "        self.observation_space = spaces.Box(low=0, high=np.inf, shape=(4,), dtype=np.float32)\n",
    "\n",
    "    def reset(self):\n",
    "        self.savings = 0\n",
    "        self.total_income = 0\n",
    "        self.total_expenses = 0\n",
    "        self.state = [self.savings, self.total_income, self.total_expenses, 0]\n",
    "        return np.array(self.state, dtype=np.float32)\n",
    "\n",
    "    def step(self, action):\n",
    "        daily_income = np.random.uniform(self.min_daily_income, self.max_daily_income)\n",
    "        daily_expenses = np.random.uniform(self.min_daily_expenses, self.max_daily_expenses)\n",
    "        \n",
    "        reward = 0\n",
    "\n",
    "        if action == 0:  # save\n",
    "            savings_amount = daily_income - daily_expenses\n",
    "            self.savings += savings_amount\n",
    "            self.total_income += daily_income\n",
    "            self.total_expenses += daily_expenses  # Update total expenses\n",
    "            reward = savings_amount\n",
    "        elif action == 1:  # spend\n",
    "            self.total_expenses += daily_expenses\n",
    "            self.total_income += daily_income\n",
    "            reward = -daily_expenses\n",
    "        elif action == 2:  # buy bag\n",
    "            if self.savings >= self.bag_price:\n",
    "                self.savings -= self.bag_price\n",
    "                reward = -self.bag_price\n",
    "            else:\n",
    "                reward = -2000\n",
    "            # Regardless of the action, update expenses for the day\n",
    "            self.total_income += daily_income\n",
    "            self.total_expenses += daily_expenses\n",
    "\n",
    "        self.state = [self.savings, self.total_income, self.total_expenses, self.savings / self.goal]\n",
    "        done = self.savings >= self.goal\n",
    "\n",
    "        return np.array(self.state, dtype=np.float32), reward, done, {}\n",
    "\n",
    "    def render(self, mode='human'):\n",
    "        output = f\"Savings: {self.savings}, Income: {self.total_income}, Expenses: {self.total_expenses}, Goal Progress: {self.savings / self.goal:.2%}\"\n",
    "        if mode == 'human':\n",
    "            print(output)\n",
    "        elif mode == 'ansi':\n",
    "            return output\n",
    "        else:\n",
    "            super().render(mode=mode)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c757687e-17dc-4099-a057-dd7f910a0003",
   "metadata": {},
   "source": [
    "## Sample Model Training and Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "56aa8c34-951f-4e53-9e99-802834b5479d",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using cpu device\n",
      "Wrapping the env with a `Monitor` wrapper\n",
      "Wrapping the env in a DummyVecEnv.\n",
      "-----------------------------------\n",
      "| rollout/            |           |\n",
      "|    ep_len_mean      | 25.5      |\n",
      "|    ep_rew_mean      | -1.78e+04 |\n",
      "|    exploration_rate | 0.515     |\n",
      "| time/               |           |\n",
      "|    episodes         | 4         |\n",
      "|    fps              | 24691     |\n",
      "|    time_elapsed     | 0         |\n",
      "|    total_timesteps  | 102       |\n",
      "-----------------------------------\n",
      "-----------------------------------\n",
      "| rollout/            |           |\n",
      "|    ep_len_mean      | 18.2      |\n",
      "|    ep_rew_mean      | -1.08e+04 |\n",
      "|    exploration_rate | 0.307     |\n",
      "| time/               |           |\n",
      "|    episodes         | 8         |\n",
      "|    fps              | 3435      |\n",
      "|    time_elapsed     | 0         |\n",
      "|    total_timesteps  | 146       |\n",
      "| train/              |           |\n",
      "|    learning_rate    | 0.0001    |\n",
      "|    loss             | 673       |\n",
      "|    n_updates        | 11        |\n",
      "-----------------------------------\n",
      "-----------------------------------\n",
      "| rollout/            |           |\n",
      "|    ep_len_mean      | 15.2      |\n",
      "|    ep_rew_mean      | -7.49e+03 |\n",
      "|    exploration_rate | 0.131     |\n",
      "| time/               |           |\n",
      "|    episodes         | 12        |\n",
      "|    fps              | 3184      |\n",
      "|    time_elapsed     | 0         |\n",
      "|    total_timesteps  | 183       |\n",
      "| train/              |           |\n",
      "|    learning_rate    | 0.0001    |\n",
      "|    loss             | 488       |\n",
      "|    n_updates        | 20        |\n",
      "-----------------------------------\n",
      "-----------------------------------\n",
      "| rollout/            |           |\n",
      "|    ep_len_mean      | 13.6      |\n",
      "|    ep_rew_mean      | -5.58e+03 |\n",
      "|    exploration_rate | 0.05      |\n",
      "| time/               |           |\n",
      "|    episodes         | 16        |\n",
      "|    fps              | 3003      |\n",
      "|    time_elapsed     | 0         |\n",
      "|    total_timesteps  | 217       |\n",
      "| train/              |           |\n",
      "|    learning_rate    | 0.0001    |\n",
      "|    loss             | 603       |\n",
      "|    n_updates        | 29        |\n",
      "-----------------------------------\n",
      "-----------------------------------\n",
      "| rollout/            |           |\n",
      "|    ep_len_mean      | 12.6      |\n",
      "|    ep_rew_mean      | -4.43e+03 |\n",
      "|    exploration_rate | 0.05      |\n",
      "| time/               |           |\n",
      "|    episodes         | 20        |\n",
      "|    fps              | 3007      |\n",
      "|    time_elapsed     | 0         |\n",
      "|    total_timesteps  | 252       |\n",
      "| train/              |           |\n",
      "|    learning_rate    | 0.0001    |\n",
      "|    loss             | 453       |\n",
      "|    n_updates        | 37        |\n",
      "-----------------------------------\n",
      "-----------------------------------\n",
      "| rollout/            |           |\n",
      "|    ep_len_mean      | 11.9      |\n",
      "|    ep_rew_mean      | -3.66e+03 |\n",
      "|    exploration_rate | 0.05      |\n",
      "| time/               |           |\n",
      "|    episodes         | 24        |\n",
      "|    fps              | 2981      |\n",
      "|    time_elapsed     | 0         |\n",
      "|    total_timesteps  | 286       |\n",
      "| train/              |           |\n",
      "|    learning_rate    | 0.0001    |\n",
      "|    loss             | 259       |\n",
      "|    n_updates        | 46        |\n",
      "-----------------------------------\n",
      "-----------------------------------\n",
      "| rollout/            |           |\n",
      "|    ep_len_mean      | 11.6      |\n",
      "|    ep_rew_mean      | -3.19e+03 |\n",
      "|    exploration_rate | 0.05      |\n",
      "| time/               |           |\n",
      "|    episodes         | 28        |\n",
      "|    fps              | 2947      |\n",
      "|    time_elapsed     | 0         |\n",
      "|    total_timesteps  | 324       |\n",
      "| train/              |           |\n",
      "|    learning_rate    | 0.0001    |\n",
      "|    loss             | 226       |\n",
      "|    n_updates        | 55        |\n",
      "-----------------------------------\n",
      "-----------------------------------\n",
      "| rollout/            |           |\n",
      "|    ep_len_mean      | 11.2      |\n",
      "|    ep_rew_mean      | -2.77e+03 |\n",
      "|    exploration_rate | 0.05      |\n",
      "| time/               |           |\n",
      "|    episodes         | 32        |\n",
      "|    fps              | 2919      |\n",
      "|    time_elapsed     | 0         |\n",
      "|    total_timesteps  | 359       |\n",
      "| train/              |           |\n",
      "|    learning_rate    | 0.0001    |\n",
      "|    loss             | 251       |\n",
      "|    n_updates        | 64        |\n",
      "-----------------------------------\n",
      "-----------------------------------\n",
      "| rollout/            |           |\n",
      "|    ep_len_mean      | 10.9      |\n",
      "|    ep_rew_mean      | -2.45e+03 |\n",
      "|    exploration_rate | 0.05      |\n",
      "| time/               |           |\n",
      "|    episodes         | 36        |\n",
      "|    fps              | 2925      |\n",
      "|    time_elapsed     | 0         |\n",
      "|    total_timesteps  | 392       |\n",
      "| train/              |           |\n",
      "|    learning_rate    | 0.0001    |\n",
      "|    loss             | 259       |\n",
      "|    n_updates        | 72        |\n",
      "-----------------------------------\n",
      "-----------------------------------\n",
      "| rollout/            |           |\n",
      "|    ep_len_mean      | 10.7      |\n",
      "|    ep_rew_mean      | -2.19e+03 |\n",
      "|    exploration_rate | 0.05      |\n",
      "| time/               |           |\n",
      "|    episodes         | 40        |\n",
      "|    fps              | 2778      |\n",
      "|    time_elapsed     | 0         |\n",
      "|    total_timesteps  | 427       |\n",
      "| train/              |           |\n",
      "|    learning_rate    | 0.0001    |\n",
      "|    loss             | 137       |\n",
      "|    n_updates        | 81        |\n",
      "-----------------------------------\n",
      "-----------------------------------\n",
      "| rollout/            |           |\n",
      "|    ep_len_mean      | 10.5      |\n",
      "|    ep_rew_mean      | -1.97e+03 |\n",
      "|    exploration_rate | 0.05      |\n",
      "| time/               |           |\n",
      "|    episodes         | 44        |\n",
      "|    fps              | 2780      |\n",
      "|    time_elapsed     | 0         |\n",
      "|    total_timesteps  | 462       |\n",
      "| train/              |           |\n",
      "|    learning_rate    | 0.0001    |\n",
      "|    loss             | 93.4      |\n",
      "|    n_updates        | 90        |\n",
      "-----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 10.4     |\n",
      "|    ep_rew_mean      | -1.8e+03 |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 48       |\n",
      "|    fps              | 2824     |\n",
      "|    time_elapsed     | 0        |\n",
      "|    total_timesteps  | 498      |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 264      |\n",
      "|    n_updates        | 99       |\n",
      "----------------------------------\n",
      "-----------------------------------\n",
      "| rollout/            |           |\n",
      "|    ep_len_mean      | 10.3      |\n",
      "|    ep_rew_mean      | -1.68e+03 |\n",
      "|    exploration_rate | 0.05      |\n",
      "| time/               |           |\n",
      "|    episodes         | 52        |\n",
      "|    fps              | 2868      |\n",
      "|    time_elapsed     | 0         |\n",
      "|    total_timesteps  | 534       |\n",
      "| train/              |           |\n",
      "|    learning_rate    | 0.0001    |\n",
      "|    loss             | 125       |\n",
      "|    n_updates        | 108       |\n",
      "-----------------------------------\n",
      "-----------------------------------\n",
      "| rollout/            |           |\n",
      "|    ep_len_mean      | 10.2      |\n",
      "|    ep_rew_mean      | -1.55e+03 |\n",
      "|    exploration_rate | 0.05      |\n",
      "| time/               |           |\n",
      "|    episodes         | 56        |\n",
      "|    fps              | 2891      |\n",
      "|    time_elapsed     | 0         |\n",
      "|    total_timesteps  | 569       |\n",
      "| train/              |           |\n",
      "|    learning_rate    | 0.0001    |\n",
      "|    loss             | 172       |\n",
      "|    n_updates        | 117       |\n",
      "-----------------------------------\n",
      "-----------------------------------\n",
      "| rollout/            |           |\n",
      "|    ep_len_mean      | 10.1      |\n",
      "|    ep_rew_mean      | -1.51e+03 |\n",
      "|    exploration_rate | 0.05      |\n",
      "| time/               |           |\n",
      "|    episodes         | 60        |\n",
      "|    fps              | 2906      |\n",
      "|    time_elapsed     | 0         |\n",
      "|    total_timesteps  | 606       |\n",
      "| train/              |           |\n",
      "|    learning_rate    | 0.0001    |\n",
      "|    loss             | 40.6      |\n",
      "|    n_updates        | 126       |\n",
      "-----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 10       |\n",
      "|    ep_rew_mean      | -1.4e+03 |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 64       |\n",
      "|    fps              | 2934     |\n",
      "|    time_elapsed     | 0        |\n",
      "|    total_timesteps  | 642      |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 170      |\n",
      "|    n_updates        | 135      |\n",
      "----------------------------------\n",
      "-----------------------------------\n",
      "| rollout/            |           |\n",
      "|    ep_len_mean      | 9.91      |\n",
      "|    ep_rew_mean      | -1.31e+03 |\n",
      "|    exploration_rate | 0.05      |\n",
      "| time/               |           |\n",
      "|    episodes         | 68        |\n",
      "|    fps              | 2939      |\n",
      "|    time_elapsed     | 0         |\n",
      "|    total_timesteps  | 674       |\n",
      "| train/              |           |\n",
      "|    learning_rate    | 0.0001    |\n",
      "|    loss             | 98.3      |\n",
      "|    n_updates        | 143       |\n",
      "-----------------------------------\n",
      "-----------------------------------\n",
      "| rollout/            |           |\n",
      "|    ep_len_mean      | 9.88      |\n",
      "|    ep_rew_mean      | -1.28e+03 |\n",
      "|    exploration_rate | 0.05      |\n",
      "| time/               |           |\n",
      "|    episodes         | 72        |\n",
      "|    fps              | 2972      |\n",
      "|    time_elapsed     | 0         |\n",
      "|    total_timesteps  | 711       |\n",
      "| train/              |           |\n",
      "|    learning_rate    | 0.0001    |\n",
      "|    loss             | 137       |\n",
      "|    n_updates        | 152       |\n",
      "-----------------------------------\n",
      "-----------------------------------\n",
      "| rollout/            |           |\n",
      "|    ep_len_mean      | 9.8       |\n",
      "|    ep_rew_mean      | -1.23e+03 |\n",
      "|    exploration_rate | 0.05      |\n",
      "| time/               |           |\n",
      "|    episodes         | 76        |\n",
      "|    fps              | 2972      |\n",
      "|    time_elapsed     | 0         |\n",
      "|    total_timesteps  | 745       |\n",
      "| train/              |           |\n",
      "|    learning_rate    | 0.0001    |\n",
      "|    loss             | 176       |\n",
      "|    n_updates        | 161       |\n",
      "-----------------------------------\n",
      "-----------------------------------\n",
      "| rollout/            |           |\n",
      "|    ep_len_mean      | 9.76      |\n",
      "|    ep_rew_mean      | -1.19e+03 |\n",
      "|    exploration_rate | 0.05      |\n",
      "| time/               |           |\n",
      "|    episodes         | 80        |\n",
      "|    fps              | 2951      |\n",
      "|    time_elapsed     | 0         |\n",
      "|    total_timesteps  | 781       |\n",
      "| train/              |           |\n",
      "|    learning_rate    | 0.0001    |\n",
      "|    loss             | 49        |\n",
      "|    n_updates        | 170       |\n",
      "-----------------------------------\n",
      "-----------------------------------\n",
      "| rollout/            |           |\n",
      "|    ep_len_mean      | 9.71      |\n",
      "|    ep_rew_mean      | -1.13e+03 |\n",
      "|    exploration_rate | 0.05      |\n",
      "| time/               |           |\n",
      "|    episodes         | 84        |\n",
      "|    fps              | 2952      |\n",
      "|    time_elapsed     | 0         |\n",
      "|    total_timesteps  | 816       |\n",
      "| train/              |           |\n",
      "|    learning_rate    | 0.0001    |\n",
      "|    loss             | 35.6      |\n",
      "|    n_updates        | 178       |\n",
      "-----------------------------------\n",
      "-----------------------------------\n",
      "| rollout/            |           |\n",
      "|    ep_len_mean      | 9.66      |\n",
      "|    ep_rew_mean      | -1.09e+03 |\n",
      "|    exploration_rate | 0.05      |\n",
      "| time/               |           |\n",
      "|    episodes         | 88        |\n",
      "|    fps              | 2951      |\n",
      "|    time_elapsed     | 0         |\n",
      "|    total_timesteps  | 850       |\n",
      "| train/              |           |\n",
      "|    learning_rate    | 0.0001    |\n",
      "|    loss             | 284       |\n",
      "|    n_updates        | 187       |\n",
      "-----------------------------------\n",
      "-----------------------------------\n",
      "| rollout/            |           |\n",
      "|    ep_len_mean      | 9.64      |\n",
      "|    ep_rew_mean      | -1.04e+03 |\n",
      "|    exploration_rate | 0.05      |\n",
      "| time/               |           |\n",
      "|    episodes         | 92        |\n",
      "|    fps              | 2966      |\n",
      "|    time_elapsed     | 0         |\n",
      "|    total_timesteps  | 887       |\n",
      "| train/              |           |\n",
      "|    learning_rate    | 0.0001    |\n",
      "|    loss             | 299       |\n",
      "|    n_updates        | 196       |\n",
      "-----------------------------------\n",
      "-----------------------------------\n",
      "| rollout/            |           |\n",
      "|    ep_len_mean      | 9.61      |\n",
      "|    ep_rew_mean      | -1.01e+03 |\n",
      "|    exploration_rate | 0.05      |\n",
      "| time/               |           |\n",
      "|    episodes         | 96        |\n",
      "|    fps              | 2978      |\n",
      "|    time_elapsed     | 0         |\n",
      "|    total_timesteps  | 923       |\n",
      "| train/              |           |\n",
      "|    learning_rate    | 0.0001    |\n",
      "|    loss             | 303       |\n",
      "|    n_updates        | 205       |\n",
      "-----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 9.57     |\n",
      "|    ep_rew_mean      | -961     |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 100      |\n",
      "|    fps              | 2959     |\n",
      "|    time_elapsed     | 0        |\n",
      "|    total_timesteps  | 957      |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 112      |\n",
      "|    n_updates        | 214      |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 8.91     |\n",
      "|    ep_rew_mean      | -243     |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 104      |\n",
      "|    fps              | 2977     |\n",
      "|    time_elapsed     | 0        |\n",
      "|    total_timesteps  | 993      |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 110      |\n",
      "|    n_updates        | 223      |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 8.82     |\n",
      "|    ep_rew_mean      | -83.6    |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 108      |\n",
      "|    fps              | 2988     |\n",
      "|    time_elapsed     | 0        |\n",
      "|    total_timesteps  | 1028     |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 21.4     |\n",
      "|    n_updates        | 231      |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 8.82     |\n",
      "|    ep_rew_mean      | -43.4    |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 112      |\n",
      "|    fps              | 3001     |\n",
      "|    time_elapsed     | 0        |\n",
      "|    total_timesteps  | 1065     |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 132      |\n",
      "|    n_updates        | 241      |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 8.84     |\n",
      "|    ep_rew_mean      | -43.2    |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 116      |\n",
      "|    fps              | 3012     |\n",
      "|    time_elapsed     | 0        |\n",
      "|    total_timesteps  | 1101     |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 82.4     |\n",
      "|    n_updates        | 250      |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 8.83     |\n",
      "|    ep_rew_mean      | -43.4    |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 120      |\n",
      "|    fps              | 3005     |\n",
      "|    time_elapsed     | 0        |\n",
      "|    total_timesteps  | 1135     |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 140      |\n",
      "|    n_updates        | 258      |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 8.86     |\n",
      "|    ep_rew_mean      | -43.1    |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 124      |\n",
      "|    fps              | 3000     |\n",
      "|    time_elapsed     | 0        |\n",
      "|    total_timesteps  | 1172     |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 42.4     |\n",
      "|    n_updates        | 267      |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 8.84     |\n",
      "|    ep_rew_mean      | -42.7    |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 128      |\n",
      "|    fps              | 3011     |\n",
      "|    time_elapsed     | 0        |\n",
      "|    total_timesteps  | 1208     |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 208      |\n",
      "|    n_updates        | 276      |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 8.84     |\n",
      "|    ep_rew_mean      | -42.5    |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 132      |\n",
      "|    fps              | 3016     |\n",
      "|    time_elapsed     | 0        |\n",
      "|    total_timesteps  | 1243     |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 48.7     |\n",
      "|    n_updates        | 285      |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 8.87     |\n",
      "|    ep_rew_mean      | -42.3    |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 136      |\n",
      "|    fps              | 3013     |\n",
      "|    time_elapsed     | 0        |\n",
      "|    total_timesteps  | 1279     |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 243      |\n",
      "|    n_updates        | 294      |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 8.86     |\n",
      "|    ep_rew_mean      | -42.6    |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 140      |\n",
      "|    fps              | 3010     |\n",
      "|    time_elapsed     | 0        |\n",
      "|    total_timesteps  | 1313     |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 31.5     |\n",
      "|    n_updates        | 303      |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 8.87     |\n",
      "|    ep_rew_mean      | -42.5    |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 144      |\n",
      "|    fps              | 3009     |\n",
      "|    time_elapsed     | 0        |\n",
      "|    total_timesteps  | 1349     |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 120      |\n",
      "|    n_updates        | 312      |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 8.88     |\n",
      "|    ep_rew_mean      | -42.5    |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 148      |\n",
      "|    fps              | 3012     |\n",
      "|    time_elapsed     | 0        |\n",
      "|    total_timesteps  | 1386     |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 42.8     |\n",
      "|    n_updates        | 321      |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 8.89     |\n",
      "|    ep_rew_mean      | -62.4    |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 152      |\n",
      "|    fps              | 3008     |\n",
      "|    time_elapsed     | 0        |\n",
      "|    total_timesteps  | 1423     |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 24.6     |\n",
      "|    n_updates        | 330      |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 8.88     |\n",
      "|    ep_rew_mean      | -62.2    |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 156      |\n",
      "|    fps              | 3012     |\n",
      "|    time_elapsed     | 0        |\n",
      "|    total_timesteps  | 1457     |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 19.4     |\n",
      "|    n_updates        | 339      |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 8.85     |\n",
      "|    ep_rew_mean      | -22.6    |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 160      |\n",
      "|    fps              | 3004     |\n",
      "|    time_elapsed     | 0        |\n",
      "|    total_timesteps  | 1491     |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 32.3     |\n",
      "|    n_updates        | 347      |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 8.82     |\n",
      "|    ep_rew_mean      | -22.8    |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 164      |\n",
      "|    fps              | 3010     |\n",
      "|    time_elapsed     | 0        |\n",
      "|    total_timesteps  | 1524     |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 24.7     |\n",
      "|    n_updates        | 355      |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 8.87     |\n",
      "|    ep_rew_mean      | -41.9    |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 168      |\n",
      "|    fps              | 3004     |\n",
      "|    time_elapsed     | 0        |\n",
      "|    total_timesteps  | 1561     |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 23.7     |\n",
      "|    n_updates        | 365      |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 8.88     |\n",
      "|    ep_rew_mean      | -21.6    |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 172      |\n",
      "|    fps              | 3006     |\n",
      "|    time_elapsed     | 0        |\n",
      "|    total_timesteps  | 1599     |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 112      |\n",
      "|    n_updates        | 374      |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 8.88     |\n",
      "|    ep_rew_mean      | -1.6     |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 176      |\n",
      "|    fps              | 3007     |\n",
      "|    time_elapsed     | 0        |\n",
      "|    total_timesteps  | 1633     |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 19.1     |\n",
      "|    n_updates        | 383      |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 8.88     |\n",
      "|    ep_rew_mean      | 18.3     |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 180      |\n",
      "|    fps              | 3013     |\n",
      "|    time_elapsed     | 0        |\n",
      "|    total_timesteps  | 1669     |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 137      |\n",
      "|    n_updates        | 392      |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 8.92     |\n",
      "|    ep_rew_mean      | -21.2    |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 184      |\n",
      "|    fps              | 3021     |\n",
      "|    time_elapsed     | 0        |\n",
      "|    total_timesteps  | 1708     |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 21.2     |\n",
      "|    n_updates        | 401      |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 8.93     |\n",
      "|    ep_rew_mean      | -0.801   |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 188      |\n",
      "|    fps              | 3026     |\n",
      "|    time_elapsed     | 0        |\n",
      "|    total_timesteps  | 1743     |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 123      |\n",
      "|    n_updates        | 410      |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 8.94     |\n",
      "|    ep_rew_mean      | -40.6    |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 192      |\n",
      "|    fps              | 3027     |\n",
      "|    time_elapsed     | 0        |\n",
      "|    total_timesteps  | 1781     |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 39.9     |\n",
      "|    n_updates        | 420      |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 8.95     |\n",
      "|    ep_rew_mean      | -40.4    |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 196      |\n",
      "|    fps              | 3030     |\n",
      "|    time_elapsed     | 0        |\n",
      "|    total_timesteps  | 1818     |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 35.6     |\n",
      "|    n_updates        | 429      |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 8.97     |\n",
      "|    ep_rew_mean      | -100     |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 200      |\n",
      "|    fps              | 3022     |\n",
      "|    time_elapsed     | 0        |\n",
      "|    total_timesteps  | 1854     |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 109      |\n",
      "|    n_updates        | 438      |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 8.96     |\n",
      "|    ep_rew_mean      | -101     |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 204      |\n",
      "|    fps              | 3024     |\n",
      "|    time_elapsed     | 0        |\n",
      "|    total_timesteps  | 1889     |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 27       |\n",
      "|    n_updates        | 447      |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 8.97     |\n",
      "|    ep_rew_mean      | -121     |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 208      |\n",
      "|    fps              | 3030     |\n",
      "|    time_elapsed     | 0        |\n",
      "|    total_timesteps  | 1925     |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 51.6     |\n",
      "|    n_updates        | 456      |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 8.98     |\n",
      "|    ep_rew_mean      | -160     |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 212      |\n",
      "|    fps              | 3035     |\n",
      "|    time_elapsed     | 0        |\n",
      "|    total_timesteps  | 1963     |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 20       |\n",
      "|    n_updates        | 465      |\n",
      "----------------------------------\n",
      "Reached 10.0% of the goal:\n",
      "Savings: 127.51, Total Income: 148.41, Total Expenses: 20.90, Goal Progress: 12.75%\n",
      "Total Reward: 12.75\n",
      "\n",
      "Savings: 127.51323579067713, Income: 148.41365910219167, Expenses: 20.900423311514537, Goal Progress: 12.75%\n",
      "Reached 20.0% of the goal:\n",
      "Savings: 260.18, Total Income: 295.49, Total Expenses: 35.32, Goal Progress: 26.02%\n",
      "Total Reward: 26.02\n",
      "\n",
      "Savings: 260.175377831372, Income: 295.49372559710673, Expenses: 35.31834776573473, Goal Progress: 26.02%\n",
      "Reached 30.000000000000004% of the goal:\n",
      "Savings: 383.32, Total Income: 438.40, Total Expenses: 55.08, Goal Progress: 38.33%\n",
      "Total Reward: 38.33\n",
      "\n",
      "Savings: 383.32373475281014, Income: 438.401301010214, Expenses: 55.077566257403845, Goal Progress: 38.33%\n",
      "Reached 40.0% of the goal:\n",
      "Savings: 492.52, Total Income: 582.08, Total Expenses: 89.55, Goal Progress: 49.25%\n",
      "Total Reward: 49.25\n",
      "\n",
      "Savings: 492.52344776726795, Income: 582.076007522224, Expenses: 89.55255975495609, Goal Progress: 49.25%\n",
      "Reached 50.0% of the goal:\n",
      "Savings: 623.41, Total Income: 726.81, Total Expenses: 103.41, Goal Progress: 62.34%\n",
      "Total Reward: 72.34\n",
      "\n",
      "Savings: 623.4064802126375, Income: 726.8132318123704, Expenses: 103.40675159973304, Goal Progress: 62.34%\n",
      "Reached 60.0% of the goal:\n",
      "Savings: 754.35, Total Income: 882.82, Total Expenses: 128.47, Goal Progress: 75.43%\n",
      "Total Reward: 95.43\n",
      "\n",
      "Savings: 754.3481053340988, Income: 882.8199551108553, Expenses: 128.4718497767566, Goal Progress: 75.43%\n",
      "Reached 70.0% of the goal:\n",
      "Savings: 868.96, Total Income: 1029.31, Total Expenses: 160.35, Goal Progress: 86.90%\n",
      "Total Reward: 116.90\n",
      "\n",
      "Savings: 868.9591889018448, Income: 1029.3086618544446, Expenses: 160.3494729526001, Goal Progress: 86.90%\n",
      "Reached 80.0% of the goal:\n",
      "Savings: 999.93, Total Income: 1182.75, Total Expenses: 182.82, Goal Progress: 99.99%\n",
      "Total Reward: 139.99\n",
      "\n",
      "Savings: 999.9255104405147, Income: 1182.7467041684968, Expenses: 182.82119372798246, Goal Progress: 99.99%\n",
      "Reached 89.99999999999999% of the goal:\n",
      "Savings: 1130.47, Total Income: 1336.87, Total Expenses: 206.40, Goal Progress: 113.05%\n",
      "Total Reward: 163.05\n",
      "\n",
      "Savings: 1130.4720835719268, Income: 1336.8676978513674, Expenses: 206.39561427944105, Goal Progress: 113.05%\n",
      "Final Total Reward: 163.05\n",
      "Final Total Reward: 163.05\n",
      "Total Evaluation Time: 0.00 seconds\n",
      "Evaluation Period: 9 days\n",
      "\n",
      "Financial Advice:\n",
      "1. You have saved 1130.47 which is 113.05% of your savings goal.\n",
      "2. Your total income over the period was RM1336.87.\n",
      "3. Your total expenses over the period were RM206.40.\n",
      "4. The evaluation period was 9 days.\n",
      "5. You do not yet have enough savings to buy the bag priced at RM2000. Continue saving to reach your goal.\n",
      "6. Congratulations! You have achieved your savings goal. Consider setting a new goal to continue building your financial security.\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import gymnasium as gym\n",
    "from gymnasium import spaces\n",
    "from stable_baselines3 import DQN\n",
    "from stable_baselines3.common.env_checker import check_env\n",
    "from gymnasium.utils import seeding\n",
    "import time\n",
    "\n",
    "class FinancialPlanningEnv(gym.Env):\n",
    "    def __init__(self, goal=1000, bag_price=2000):\n",
    "        super(FinancialPlanningEnv, self).__init__()\n",
    "        self.state = None\n",
    "        self.savings = 0\n",
    "        self.total_income = 0\n",
    "        self.total_expenses = 0\n",
    "        self.goal = goal\n",
    "        self.bag_price = bag_price\n",
    "        self.max_daily_income = 159\n",
    "        self.min_daily_income = 136\n",
    "        self.max_daily_expenses = 35\n",
    "        self.min_daily_expenses = 12\n",
    "        self.action_space = spaces.Discrete(3)  # 0: save, 1: spend, 2: buy bag\n",
    "        self.observation_space = spaces.Box(low=0, high=np.array([np.inf, np.inf, np.inf, 1]), dtype=np.float32)\n",
    "        self.max_steps = 365  # Maximum steps per episode, e.g., 1 year\n",
    "        self.current_step = 0\n",
    "\n",
    "    def reset(self, seed=None, options=None):\n",
    "        super().reset(seed=seed)\n",
    "        self.savings = 0\n",
    "        self.total_income = 0\n",
    "        self.total_expenses = 0\n",
    "        self.state = [self.savings, self.total_income, self.total_expenses, 0]\n",
    "        self.current_step = 0\n",
    "        if seed is not None:\n",
    "            self.np_random, _ = seeding.np_random(seed)\n",
    "        return np.array(self.state, dtype=np.float32), {}\n",
    "\n",
    "    def step(self, action):\n",
    "        daily_income = np.random.uniform(self.min_daily_income, self.max_daily_income)\n",
    "        daily_expenses = np.random.uniform(self.min_daily_expenses, self.max_daily_expenses)\n",
    "        \n",
    "        reward = 0\n",
    "\n",
    "        if action == 0:  # save\n",
    "            savings_amount = daily_income - daily_expenses\n",
    "            self.savings += savings_amount\n",
    "            self.total_income += daily_income\n",
    "            self.total_expenses += daily_expenses  # Update total expenses\n",
    "            reward = savings_amount * 0.1  # Smaller incremental reward for saving\n",
    "        elif action == 1:  # spend\n",
    "            self.total_expenses += daily_expenses\n",
    "            self.total_income += daily_income\n",
    "            reward = -daily_expenses * 0.1  # Smaller penalty for spending\n",
    "        elif action == 2:  # buy bag\n",
    "            if self.savings >= self.bag_price:\n",
    "                self.savings -= self.bag_price\n",
    "                reward = 1000  # Reward for buying the bag\n",
    "            else:\n",
    "                reward = -2000  # Penalty for attempting to buy without enough savings\n",
    "            self.total_income += daily_income\n",
    "            self.total_expenses += daily_expenses\n",
    "\n",
    "        # Continuous reward for maintaining savings\n",
    "        if self.savings >= 0.5 * self.goal:  # Reward for maintaining at least 50% of the goal\n",
    "            reward += 10\n",
    "        \n",
    "        self.state = [self.savings, self.total_income, self.total_expenses, self.savings / self.goal]\n",
    "        done = self.savings >= self.goal\n",
    "        self.current_step += 1\n",
    "        truncated = self.current_step >= self.max_steps\n",
    "\n",
    "        return np.array(self.state, dtype=np.float32), reward, done, truncated, {}\n",
    "\n",
    "    def render(self, mode='human'):\n",
    "        output = f\"Savings: {self.savings}, Income: {self.total_income}, Expenses: {self.total_expenses}, Goal Progress: {self.savings / self.goal:.2%}\"\n",
    "        if mode == 'human':\n",
    "            print(output)\n",
    "        elif mode == 'ansi':\n",
    "            return output\n",
    "        else:\n",
    "            super().render(mode=mode)\n",
    "\n",
    "# Initialize the environment\n",
    "env = FinancialPlanningEnv(goal=1000, bag_price=2000)\n",
    "\n",
    "# Check the environment\n",
    "check_env(env)\n",
    "\n",
    "# Create the RL model\n",
    "model = DQN('MlpPolicy', env, verbose=1)\n",
    "\n",
    "# Train the model\n",
    "model.learn(total_timesteps=2000)\n",
    "\n",
    "# Save the model\n",
    "model.save(\"financial_planning_dqn\")\n",
    "\n",
    "# Load the trained model\n",
    "model = DQN.load(\"financial_planning_dqn\")\n",
    "\n",
    "# Evaluate the trained model\n",
    "state, _ = env.reset()\n",
    "done = False\n",
    "total_reward = 0\n",
    "next_milestone = 0.1  # Next milestone is 10% progress\n",
    "start_time = time.time()\n",
    "\n",
    "steps_taken = 0  # To keep track of the number of steps taken\n",
    "\n",
    "while not done:\n",
    "    action, _states = model.predict(state)\n",
    "    state, reward, done, truncated, _ = env.step(action)\n",
    "    total_reward += reward\n",
    "    steps_taken += 1  # Increment the step counter\n",
    "\n",
    "    # Check if we've reached the next milestone\n",
    "    if state[3] >= next_milestone:\n",
    "        print(f\"Reached {next_milestone * 100}% of the goal:\")\n",
    "        print(f\"Savings: {state[0]:.2f}, Total Income: {state[1]:.2f}, Total Expenses: {state[2]:.2f}, Goal Progress: {state[3] * 100:.2f}%\")\n",
    "        print(f\"Total Reward: {total_reward:.2f}\\n\")\n",
    "        \n",
    "        # Update the next milestone\n",
    "        next_milestone += 0.1\n",
    "    \n",
    "    # Render only at milestone checks\n",
    "    if state[3] >= next_milestone - 0.1:\n",
    "        env.render()\n",
    "\n",
    "end_time = time.time()\n",
    "\n",
    "print(f\"Final Total Reward: {total_reward:.2f}\")\n",
    "\n",
    "# Final output as financial advice\n",
    "final_savings = state[0]\n",
    "final_income = state[1]\n",
    "final_expenses = state[2]\n",
    "goal_progress = state[3] * 100\n",
    "\n",
    "# Calculate the period in days\n",
    "evaluation_period_days = steps_taken\n",
    "\n",
    "print(f\"Final Total Reward: {total_reward:.2f}\")\n",
    "print(f\"Total Evaluation Time: {end_time - start_time:.2f} seconds\")\n",
    "print(f\"Evaluation Period: {evaluation_period_days} days\")\n",
    "print(\"\\nFinancial Advice:\")\n",
    "print(f\"1. You have saved {final_savings:.2f} which is {goal_progress:.2f}% of your savings goal.\")\n",
    "print(f\"2. Your total income over the period was RM{final_income:.2f}.\")\n",
    "print(f\"3. Your total expenses over the period were RM{final_expenses:.2f}.\")\n",
    "print(f\"4. The evaluation period was {evaluation_period_days} days.\")\n",
    "if final_savings >= env.bag_price:\n",
    "    print(f\"5. You have enough savings to buy the bag priced at RM{env.bag_price}. Consider making the purchase if it aligns with your goals.\")\n",
    "else:\n",
    "    print(f\"5. You do not yet have enough savings to buy the bag priced at RM{env.bag_price}. Continue saving to reach your goal.\")\n",
    "if goal_progress >= 100:\n",
    "    print(\"6. Congratulations! You have achieved your savings goal. Consider setting a new goal to continue building your financial security.\")\n",
    "else:\n",
    "    print(\"6. You are making progress towards your savings goal. Keep up the good work and continue to monitor your spending and saving habits.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef475b5f-cbd5-41c5-b8f8-c97acf77e8bb",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d08d1ad9-4a80-4e5f-99ae-cebc526a67ba",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:AYS]",
   "language": "python",
   "name": "conda-env-AYS-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
